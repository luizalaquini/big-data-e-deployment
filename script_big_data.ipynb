{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNIVERSIDADE DE SÃO PAULO**<br>\n",
    "**MBA DATA SCIENCE & ANALYTICS USP/ESALQ**<br>\n",
    "**BIG DATA E DEPLOYMENT DE MODELOS**<br>\n",
    "**Prof. Helder Prado Santos**<br>\n",
    "Aluna: Luiza Batista Laquini<br>\n",
    "Turma: DSA 241<br>\n",
    "\n",
    "*coding: utf-8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instalação dos pacotes necessário\n",
    "# !pip install pyspark==3.5.1\n",
    "# !pip install findspark\n",
    "# !pip install seaborn\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Importando as bibliotecas que utilizaremos durante a aula\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encontrando as configurações para inicializar o Spark\n",
    "findspark.init()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Nome da aplicação Spark\n",
    "app_name = \"Aplicação Spark\"\n",
    "\n",
    "# Inicializando o objeto de configurações do Spark\n",
    "conf = SparkConf()\n",
    "\n",
    "# Configuração do tipo de cluster\n",
    "conf.set(\"spark.master\", \"local[*]\")\n",
    "# Definição do nome da aplicação\n",
    "conf.set(\"spark.app.name\", app_name)\n",
    "# Definição da quantidade de núcleos de processamento a serem usados\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "# Definição da quantidade de memória alocada para cada executor\n",
    "conf.set(\"spark.executor.memory\", \"2g\")\n",
    "conf.set(\"spark.driver.memory\", \"2g\")\n",
    "\n",
    "# Criando a sessão Spark\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "print(\"Spark inicializado com sucesso!\")\n",
    "\n",
    "# %%\n",
    "\n",
    "###############################################################################\n",
    "#                              DATA WRANGLING                                 #\n",
    "###############################################################################\n",
    "\n",
    "# FONTE: https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cance\n",
    "# llation-data-2009-2018\n",
    "\n",
    "# Lendo um DataFrame Spark a partir de um arquivo CSV\n",
    "df = spark.read.csv(\"./datasets/2016.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Exibindo os dados do DataFrame\n",
    "df.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Exibindo uma linha do DataFrame em formato vertical\n",
    "df.show(1, vertical=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Exibindo o esquema do DataFrame (nome das colunas e tipos)\n",
    "df.printSchema()\n",
    "\n",
    "# FL_DATE                      | DATA_VOO\n",
    "# OP_CARRIER                   | TRANSPORTADORA\n",
    "# OP_CARRIER_FL_NUM            | NUM_VOO_TRANSPORTADORA\n",
    "# ORIGIN                       | ORIGEM\n",
    "# DEST                         | DESTINO\n",
    "# CRS_DEP_TIME                 | HORARIO_PARTIDA_PROGRAMADO\n",
    "# DEP_TIME                     | HORARIO_PARTIDA_REAL\n",
    "# DEP_DELAY                    | ATRASO_PARTIDA\n",
    "# TAXI_OUT                     | TAXI_OUT\n",
    "# WHEELS_OFF                   | HORARIO_DECOLAGEM\n",
    "# WHEELS_ON                    | HORARIO_POUSO\n",
    "# TAXI_IN                      | TAXI_IN\n",
    "# CRS_ARR_TIME                 | HORARIO_CHEGADA_PROGRAMADO\n",
    "# ARR_TIME                     | HORARIO_CHEGADA_REAL\n",
    "# ARR_DELAY                    | ATRASO_CHEGADA\n",
    "# CANCELLED                    | CANCELADO\n",
    "# CANCELLATION_CODE            | CODIGO_CANCELAMENTO\n",
    "# DIVERTED                     | DESVIADO\n",
    "# CRS_ELAPSED_TIME             | TEMPO_DE_VOO_PROGRAMADO\n",
    "# ACTUAL_ELAPSED_TIME          | TEMPO_DE_VOO_REAL\n",
    "# AIR_TIME                     | TEMPO_DE_VOO_EM_AR\n",
    "# DISTANCE                     | DISTANCIA\n",
    "# CARRIER_DELAY                | ATRASO_DA_TRANSPORTADORA\n",
    "# WEATHER_DELAY                | ATRASO_DE_CLIMA\n",
    "# NAS_DELAY                    | ATRASO_NAS\n",
    "# SECURITY_DELAY               | ATRASO_DE_SEGURANCA\n",
    "# LATE_AIRCRAFT_DELAY          | ATRASO_DE_AERONAVE_TARDE\n",
    "\n",
    "# %%\n",
    "\n",
    "# Criando um DataFrame com colunas selecionadas\n",
    "df_selecionado = df.select([\"DISTANCE\", \"ACTUAL_ELAPSED_TIME\", \"ARR_DELAY\"])\n",
    "\n",
    "# Exibindo o DataFrame com as colunas selecionadas\n",
    "df_selecionado.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Visualizando as análises univariadas\n",
    "df_selecionado.describe().show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# contagem de linhas do dataset\n",
    "total_linhas = df.count()\n",
    "\n",
    "print(f\"Total de linhas no dataset: {total_linhas} \\n\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# verificando o número de partições utilizadas no processamento distribuído\n",
    "df.rdd.getNumPartitions()\n",
    "\n",
    "# %%\n",
    "\n",
    "# coletando uma pequena amostra equivalente a 1%\n",
    "pequena_amostra = df.sample(fraction=0.01)\n",
    "\n",
    "# %%\n",
    "\n",
    "# contagem de linhas do dataset reduzido\n",
    "total_linhas_pequena_amostra = pequena_amostra.count()\n",
    "\n",
    "print(f\"Total de linhas no dataset: {total_linhas_pequena_amostra} \\n\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# estratégia da extrapolação para calcular grandes volumes de dados\n",
    "total_linhas_extrapoladas = int(total_linhas_pequena_amostra / 0.01)\n",
    "\n",
    "print(f\"Total de linhas no dataset: {total_linhas_extrapoladas} \\n\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Tratando dados temporais\n",
    "\n",
    "#  Importando funções úteis do PySpark para tratamento de dados\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Documentação: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "\n",
    "# Extraindo o ano da variável FL_DATE\n",
    "df = df.withColumn(\"ano\", F.year(df[\"FL_DATE\"]))\n",
    "\n",
    "# Extraindo o mês da variável FL_DATE\n",
    "df = df.withColumn(\"mes\", F.month(df[\"FL_DATE\"]))\n",
    "\n",
    "# Extraindo o dia do mês da variável FL_DATE\n",
    "df = df.withColumn(\"dia\", F.dayofmonth(df[\"FL_DATE\"]))\n",
    "\n",
    "# %%\n",
    "\n",
    "# Exibindo os dados com as novas colunas\n",
    "df.show(5, vertical=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Removendo colunas indesejadas com o método drop\n",
    "df = df.drop(\"Unnamed: 27\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Exibindo os dados após a exclusão da coluna\n",
    "df.show(5, vertical=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Selecionando apenas as colunas temporais\n",
    "df.select(\"FL_DATE\", \"ano\", \"mes\", \"dia\").show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Verificando novamente o esquema dos dados\n",
    "df.printSchema()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Verificando valores nulos no dataset\n",
    "# Lógica: contagem quando cada linha da coluna é um valor nulo\n",
    "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show(\n",
    "    vertical=True\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Carregando os dados das transportadoras\n",
    "df_transportadoras = spark.read.csv(\n",
    "    \"./datasets/transportadoras.csv\", header=True\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Visualizando as lindas e colunas do dataset\n",
    "df_transportadoras.show(truncate=False)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Vamos verificar novamente os dado do nosso banco de dados\n",
    "df.show(1, vertical=True, truncate=False)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Renomeando os nomes das colunas para igualar os dados dos dataframes\n",
    "df_transportadoras = df_transportadoras.withColumnRenamed(\n",
    "    \"IATA Code\", \"OP_CARRIER\"\n",
    ")\n",
    "\n",
    "# Verificando novamente o esquema dos dados\n",
    "df_transportadoras.printSchema()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Tipos de joins: inner, cross, outer,full, full_outer, left, left_outer,\n",
    "# right, right_outer,left_semi, and left_anti.\n",
    "\n",
    "# Unindo o dataframe original com o dataframe das transportados na coluna\n",
    "# OP_CARRIER no tipo left (A <- B)\n",
    "\n",
    "df = df.join(df_transportadoras, on=\"OP_CARRIER\", how=\"left\").drop(\n",
    "    \"OP_CARRIER\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Verificando o schema dos dados após o junção\n",
    "df.printSchema()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Verificando os nomes das transportadoras no dataframe original\n",
    "df.show(vertical=True, truncate=False)\n",
    "\n",
    "# %%\n",
    "\n",
    "###############################################################################\n",
    "#                       AGRUPAMENTO E RESUMO DE DADOS                         #\n",
    "###############################################################################\n",
    "\n",
    "# %%\n",
    "\n",
    "# Fluxo de funções: groupby -> agg\n",
    "\n",
    "# Agrupando por mes e agregando pela média do atraso na chegada\n",
    "df.groupby(\"mes\").agg(F.avg(\"ARR_DELAY\").alias(\"media_atraso_chegada\")).show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Agrupando e resumindo múltiplas colunas\n",
    "df_agrupado = df.groupby(\"mes\").agg(\n",
    "    # média do atraso na chegada\n",
    "    F.avg(\"ARR_DELAY\").alias(\"media_atraso_chegada\"),\n",
    "    # soma de voos cancelados\n",
    "    F.sum(\"CANCELLED\").alias(\"voos_cancelados\"),\n",
    "    # contagem da quantidade de voos (linhas)\n",
    "    F.count(F.lit(1)).alias(\"quantidade_de_voos\"),\n",
    ")\n",
    "\n",
    "# Visualizar o dataset agrupado\n",
    "df_agrupado.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Criando uma nova coluna com a porcentagem dos voos\n",
    "df_agrupado = df_agrupado.withColumn(\n",
    "    \"porcentagem_voos_cancelados\",\n",
    "    # fórmula: (voos_cancelados/quantidade_de_voos)*100\n",
    "    (df_agrupado[\"voos_cancelados\"] / df_agrupado[\"quantidade_de_voos\"]) * 100,\n",
    ")\n",
    "\n",
    "# Visualizar o dataset agrupado\n",
    "df_agrupado.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Filtrando apenas os dados dos voos cancelados\n",
    "df_cancelados = df.where(df[\"CANCELLED\"] == 1)\n",
    "\n",
    "# Mostrando os dados de forma vertical\n",
    "df_cancelados.show(1, vertical=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Códigos de cancelamento\n",
    "\n",
    "# A = Por transportadora\n",
    "# B = Devido às condições climáticas\n",
    "# C = Pelo sistema nacional de transporte aéreo\n",
    "# D = Por razões de segurança\n",
    "\n",
    "# Trocando os dados pelas informações reais\n",
    "df_cancelados = df_cancelados.withColumn(\n",
    "    \"CANCELLATION_CODE\",\n",
    "    F.when(\n",
    "        df_cancelados[\"CANCELLATION_CODE\"] == \"A\", \"Por transportadora\"\n",
    "    ).otherwise(df_cancelados[\"CANCELLATION_CODE\"]),\n",
    ")\n",
    "df_cancelados = df_cancelados.withColumn(\n",
    "    \"CANCELLATION_CODE\",\n",
    "    F.when(\n",
    "        df_cancelados[\"CANCELLATION_CODE\"] == \"B\",\n",
    "        \"Devido às condições climáticas\",\n",
    "    ).otherwise(df_cancelados[\"CANCELLATION_CODE\"]),\n",
    ")\n",
    "df_cancelados = df_cancelados.withColumn(\n",
    "    \"CANCELLATION_CODE\",\n",
    "    F.when(\n",
    "        df_cancelados[\"CANCELLATION_CODE\"] == \"C\",\n",
    "        \"Pelo sistema nacional de transporte aéreo\",\n",
    "    ).otherwise(df_cancelados[\"CANCELLATION_CODE\"]),\n",
    ")\n",
    "df_cancelados = df_cancelados.withColumn(\n",
    "    \"CANCELLATION_CODE\",\n",
    "    F.when(\n",
    "        df_cancelados[\"CANCELLATION_CODE\"] == \"D\", \"Por razões de segurança\"\n",
    "    ).otherwise(df_cancelados[\"CANCELLATION_CODE\"]),\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Verificando os dados alterados\n",
    "df_cancelados.show(vertical=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Renomear coluna para melhor entendimento\n",
    "df_cancelados = df_cancelados.withColumnRenamed(\n",
    "    \"CANCELLATION_CODE\", \"motivo_cancelamento\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Agrupar os dados por motivo do cancelamento e a quantidade de voos\n",
    "# relacionados\n",
    "df_voos_cancelados_codigos = (\n",
    "    # forma de agrupamento\n",
    "    df_cancelados.groupby(\"motivo_cancelamento\")\n",
    "    # forma de resumir os dados\n",
    "    .count()\n",
    "    # renomeando a nova coluna criada\n",
    "    .withColumnRenamed(\"count\", \"quantidade\")\n",
    ")\n",
    "\n",
    "# Visulizar os dados\n",
    "df_voos_cancelados_codigos.show(truncate=False)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Passando as informações do cluster Spark para o kernel do python e criando um\n",
    "# dataframe pandas\n",
    "pdf_voos_cancelados_codigos = df_voos_cancelados_codigos.toPandas()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Agora o dataframe pode ser tratado como um dataframe pandas\n",
    "\n",
    "# Visualizando as primeiras linhas do dataset\n",
    "pdf_voos_cancelados_codigos.head()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Visualizando as variáveis univiaradas\n",
    "pdf_voos_cancelados_codigos.describe()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Verificando o tipo do dataframe pyspark\n",
    "print(type(df_voos_cancelados_codigos))\n",
    "\n",
    "# %%\n",
    "\n",
    "# Verificando o tipo do dataframe pandas\n",
    "print(type(pdf_voos_cancelados_codigos))\n",
    "\n",
    "# %%\n",
    "\n",
    "###############################################################################\n",
    "#                         CRIAÇÃO DE GRÁFICOS                                 #\n",
    "###############################################################################\n",
    "\n",
    "# criando um gráfico de barras com os dados retornados\n",
    "\n",
    "# definindo o tamanho da imagem\n",
    "plt.figure(figsize=(16,12), dpi=300) \n",
    "\n",
    "# criando o gráfico de colunas\n",
    "ax = sns.barplot(\n",
    "    pdf_voos_cancelados_codigos.sort_values(by=\"quantidade\", ascending=False),\n",
    "    x=\"motivo_cancelamento\",\n",
    "    y=\"quantidade\",\n",
    "    color=\"#8e44ad\"\n",
    ")\n",
    "# alterando título e os rótuos dos eixos x e y\n",
    "ax.set_ylabel(\"Quantidade voos cancelados\", fontsize=14)\n",
    "ax.set_xlabel(\"Motivo dos cancelamentos\", fontsize=14)\n",
    "ax.set_title(\"Voos cancelados e seus motivos\", fontsize=14)\n",
    "\n",
    "# adicionando os labels nas colunas\n",
    "ax.bar_label(ax.containers[0], fontsize=12)\n",
    "\n",
    "# plotar o gráfico\n",
    "plt.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "# verificando dados das transportadoras\n",
    "\n",
    "df_agrupado_companhias = df.groupby(\"Air Carrier Name\").agg(\n",
    "    # resumir o dado pela média do atraso na chegada e contagem de voos cancelados e total\n",
    "    # média atraso na chegada\n",
    "    F.avg(\"ARR_DELAY\").alias(\"media_atraso\"),\n",
    "    # somatório de voos cancelados\n",
    "    F.sum(\"CANCELLED\").alias(\"voos_cancelados\"),\n",
    "    # contagem da quantidade de voos\n",
    "    F.count(F.lit(1)).alias(\"quantidade_de_voos\"),\n",
    ")\n",
    "\n",
    "# mostrar os dados\n",
    "df_agrupado_companhias.show()\n",
    "\n",
    "#%%\n",
    "\n",
    "# passando as informações do driver para a memória do kernel e criando um dataframe pandas\n",
    "pdf_agrupado_companhias = df_agrupado_companhias.toPandas()\n",
    "\n",
    "# ajustando os dados para organizar os valores pela media do atraso\n",
    "pdf_agrupado_companhias = pdf_agrupado_companhias.sort_values(\n",
    "    by=\"media_atraso\", ascending=True\n",
    ").round(1)\n",
    "\n",
    "#%%\n",
    "\n",
    "# criando gráfico com os dados retornados\n",
    "\n",
    "# definindo o tamanho da imagem\n",
    "plt.figure(figsize=(16, 10), dpi=300)\n",
    "\n",
    "# criando o gráfico de colunas\n",
    "ax = sns.barplot(pdf_agrupado_companhias,\n",
    "                 color=\"#8e44ad\",\n",
    "                 x=\"Air Carrier Name\", \n",
    "                 y=\"media_atraso\")\n",
    "\n",
    "# alterando título e os rótuos dos eixos x e y\n",
    "ax.set_ylabel(\"Média de atraso (min)\", fontsize=14)\n",
    "ax.set_xlabel(\"Código da trasportadora\", fontsize=14)\n",
    "ax.set_title(\"Média de atraso nas transportadoras\", fontsize=14)\n",
    "\n",
    "# adicionando os labels nas colunas\n",
    "ax.bar_label(ax.containers[0], fontsize=12)\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# adicionar linha auxiliar no eixo y = 0\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "# plotar o gráfico\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "###############################################################################\n",
    "#                    LENDO MÚLTIPLOS DADOS ESTRUTURADOS                       #\n",
    "###############################################################################\n",
    "\n",
    "# Coletar a informação do diretório atual#\n",
    "caminho_atual = os.getcwd()\n",
    "\n",
    "# Lista de anos para carregar os dados\n",
    "lista_anos = [\"2016\", \"2017\"]\n",
    "\n",
    "# Lista que vai receber os caminhos completos\n",
    "caminho_arquivos = []\n",
    "\n",
    "# Percorrer cada ano da lista de anos\n",
    "for ano in lista_anos:\n",
    "    caminho = f\"{caminho_atual}/datasets/{ano}.csv\"\n",
    "    caminho_arquivos.append(caminho)\n",
    "\n",
    "print(caminho_arquivos)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Criar um dataframe com múltiplas fontes de bases de dados\n",
    "df = spark.read.csv(caminho_arquivos, header=True, inferSchema=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Contagem de linhas do dataframe criado\n",
    "df.count()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Coletando o ano da variável FL_DATE\n",
    "df = df.withColumn(\"ano\", F.year(df[\"FL_DATE\"]))\n",
    "\n",
    "# Coletando o mês da variável FL_DATE\n",
    "df = df.withColumn(\"mes\", F.month(df[\"FL_DATE\"]))\n",
    "\n",
    "# %%\n",
    "\n",
    "# Agrupando e resumindo agora os dados por ano\n",
    "df.groupby(\"ano\").agg(F.avg(\"ARR_DELAY\").alias(\"media_atraso\")).show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Agrupando e resumindo agora os dados por ano e por mês\n",
    "df.groupby(\"ano\", \"mes\").agg(\n",
    "    F.avg(\"ARR_DELAY\").alias(\"media_atraso\")\n",
    ").orderBy(\"ano\", \"mes\").show()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Retirando o dataframe spark da memória e do disco\n",
    "df.unpersist()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Finalizando a aplicação Spark\n",
    "spark.stop()\n",
    "\n",
    "# %%\n",
    "\n",
    "# ############################## FIM DO SCRIPT ################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
